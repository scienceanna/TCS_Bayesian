---
title: "Implementing Buetti et al(2019)"
author: "ADF Clarke and AE Hughes"
date: "24/08/2020"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
   fig.height = 3,
  fig.align = "center")
```

# Setup and Data Import

```{r load-packages, include = FALSE}
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(magrittr)
library(latex2exp)
library(corrplot)
```

```{r}
# set ggplot2 theme
theme_set(see::theme_abyss())

# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())

# functions used for the analysis reimplementation
source("scripts/reimplementation.R")

# functions used for our Bayesian re-analysis
source("scripts/our_functions.R")

```

## Importing

We will import all experiments. While we're at it, we will remove error trials and very very short responses.

*What about very very long RTs? I'm not sure if there is a good reason to, or obvious cut off? Unlike for short Rts, in which values == 0 are mathematically impossible, and values <10ms are implausible. 

We'll make a few minor changes to the data:
  - Switch from ms to seconds
  - Recode Experiment 1a and 1b to Experiment 1, Experiment 2a, 2b and 2c to Experiment 2, etc.
  - Remove the bottom and top 1% of data
  
  
```{r import-data}
source("scripts/import_and_tidy.R")
```

Do we want to register any other exclusion criteria? 
- min accuracy per participant?
- overly long RTs? 
- overly slow participants?
- incomplete data (i.e., did not complete the experiment)

[we would expect the data to be noisier due to online... so should build in some data quality checks]

# Computational Replication of Buetti et al (2019)

Before doing anything else (i.e., new), we want to confirm that we can replicate the original analysis.

## Calculating $D$

Calculate $D_e$ for each condition in each experiment. 

```{r}
De <- map_dfr(unique(d$exp_id), calc_D_per_feature)
```

## Predicing $D$

We can now predict values of $D$ for all experiments 2x and 4x. 

```{r}
Dp <- map_df(c(2,4), gen_exp_predictions, De)
```
 
Now, recreate the scatter plots from Buetti et al (2019), Figure 4.

```{r replicating-D-plot, fig.cap="Computational replication of Figure 4 (top row) from Buetti et al (2019)."}
left_join(Dp, De, by = c("exp_id", "d_feature")) %>%
  pivot_longer(
    cols = c(`best feature`, `orthog. contrast`, collinear),
    values_to = "Dp",
    names_to = "method") %>%
  mutate(method = fct_relevel(method, "best feature", "orthog. contrast")) %>%
  ggplot(aes(x = Dp, y = D)) +
  geom_point( color = "yellow1") + 
  geom_abline(linetype = 2) +
  geom_smooth(method = "lm", formula = y ~ x, colour = "violetred3") + 
  # coord_cartesian(xlim = c(0, 80), ylim = c(0, 80)) + 
  facet_wrap(~ method) + 
  scale_x_continuous("predicted D") + 
  scale_y_continuous("empirical D")
```

## Predicting Reaction Times

$L$ indicates the number of distractor types present in the display, $N_T$ is the total number of distractors, $N_i$ is the number of distractors of type $i$, $D_j$ indicates the logarithmic slope parameters associated with distractor of type $j$ (organized from smallest $D_1$ to largest $D_L$). Note that the $D$ parameter is the one that increases with increasing target-distractor similarity.

The constant a represents the reaction time when the target is alone in the display. Inter-item interactions were indexed by the multiplicative factor $\beta$. Finally, the index function $1_{[2, \infty)} (j)$ indicates that the sum over Ni only applies when there are at least two different types of lures in the display $(j > 1)$. When $j = 1$, the second sum is zero.

```{r pred-rt-replication, fig.width = 4, fig.cap="Computational Replication of RT predictions."}
exps2predict = c(2, 4)

rt_pred <- map_dfr(exps2predict, predict_rt)

d %>% filter(exp_id %in% exps2predict) %>%
	group_by(exp_id, p_id, d_feature, N_T) %>%
	summarise(mean_rt = mean(rt), .groups = "drop") %>%
	group_by(exp_id,  d_feature, N_T) %>%
	summarise(mean_rt = mean(mean_rt), .groups = "drop") %>%
	left_join(rt_pred, by = c("exp_id", "d_feature", "N_T")) %>% 
	ggplot(aes(x = p_rt, y = mean_rt)) + 
  geom_point(color = "yellow1", alpha = 0.5) + 
  geom_abline() + 
  geom_smooth(method = "lm", formula = y ~ x, colour = "violetred3") + 
  scale_x_continuous("predicted reaction time (ms)") +
  scale_y_continuous("empirical mean reaction time (ms)")
```
```{r tidy-up}
# remove variables we no longer need
rm(De, Dp, rt_pred, exps2predict, 
   calc_D_per_feature, extract_a_value, predict_D_overall, gen_exp_predictions, extract_D, predict_rt)
```

# Switching to a Bayesian Multi-Level Framework

Now that we have verified that we can re-create the original results, we switch to a Bayesian multi-level framework. We make the following important changes:

  - Modelling trial data, rather than pooled mean reaction time data. This allows us to fit a model that can generate data at the trial level, and account 
  - Use a lognormal distribution for modelling reaction times, This allows us to avoid ever predicing impossible negative reaction times. It also helps us to account for the skew in the distribution
  - We will switch from using milli-seconds to seconds. This leaves us with most values around 0.5-1seconds, which will help model fitting. I.e., a more standardised scale. 
  

## Measuring $D$ from Empirical Data


### Prior Predictions

We can take the prior model, and then use it to compute our prior predictions!

```{r glmm-prior-sample}

  prior_model_nrl <- readRDS("models/prior_nrl.models")
  prior_model_log <- readRDS("models/prior_log.models")
  prior_model_sft <- readRDS("models/prior_sft.models")

```

```{r plt-prior, fig.height=3, fig.cap="Sample Prior predictions for reaction time and log(N+1)."}

plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
plt_log <- plot_model_fits_rt(1, prior_model_log, y_limits = c(0, 10), feature2plot = "blue")
plt_sft <- plot_model_fits_rt(1, prior_model_sft, y_limits = c(0, 10), feature2plot = "blue")
                             
plt_nrl + plt_log + plt_sft

# tidy up, we no longer need to keep hold of these models and plots
rm(
 prior_model_nrl, 
 prior_model_log, 
 prior_model_sft, 
 plt_nrl, plt_log, plt_sft)
```

### Fit Model and Posterior Predictions For Experiment 1

```{r glmm-compute-post-1}

m_exp1_nrl <- readRDS("models/exp_1_nrl.models")
m_exp1_log <- readRDS("models/exp_1_log.models")
m_exp1_sft <- readRDS("models/exp_1_sft.models")

```

Calculate model weights to decide on best model...


```{r model-weights, cache = TRUE}

loo_m_exp1_nrl <- readRDS("models/loo_m_exp1_nrl.rds")
loo_m_exp1_log <- readRDS("models/loo_m_exp1_log.rds")
loo_m_exp1_sft <- readRDS("models/loo_m_exp1_sft.rds")

loo_list <- list(loo_m_exp1_nrl, loo_m_exp1_log, loo_m_exp1_sft)
loo_model_weights(loo_list)

```

The plots before show our estimates for average participant to lie. The regions illustrate the distribution of reaction times estimated by our model. 

```{r plt-post, fig.height=7, fig.cap="Posterior predictions for reaction time and log(N+1)."}

plt_nrl <- plot_model_fits_rt(1, m_exp1_nrl, plot_type = "fitted")
plt_log <- plot_model_fits_rt(1, m_exp1_log, plot_type = "fitted")
plt_sft <- plot_model_fits_rt(1, m_exp1_sft, plot_type = "fitted")

plt_nrl / plt_log / plt_sft

rm(plt_nrl, plt_log, plt_sft)
```

```{r plt-post-re, fig.height=7, fig.cap="Posterior predictions for reaction time and log(N+1)."}

plt_nrl <- plot_model_fits_rt(1, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
plt_log <- plot_model_fits_rt(1, m_exp1_log, plot_type = "predicted", y_limits = c(0, 2.5))
plt_sft <- plot_model_fits_rt(1, m_exp1_sft, plot_type = "predicted", y_limits = c(0, 2.5))

plt_nrl / plt_log / plt_sft

rm(plt_nrl, plt_log, plt_sft)
```

### Fit Model and Posterior Predictions For Experiment 2, 3, and 4

Now that we have decided what the best distribution to use is, we will fit the model to the data from Experiment 2. 

```{r glmm-compute-post-234}

m_exp2_nrl <- readRDS("models/exp_2_nrl.models")
m_exp2_log <- readRDS("models/exp_2_log.models")
m_exp2_sft <- readRDS("models/exp_2_sft.models")
m_exp3_nrl <- readRDS("models/exp_3_nrl.models")
m_exp3_log <- readRDS("models/exp_3_log.models")
m_exp3_sft <- readRDS("models/exp_3_sft.models")
m_exp4_nrl <- readRDS("models/exp_4_nrl.models")
m_exp4_log <- readRDS("models/exp_4_log.models")
m_exp4_sft <- readRDS("models/exp_4_sft.models")

```

### Use these models to compute $D$

note, remove ` subset = 1:1000` when we've re-ran models with equal number of 
samples! (in the `extract_fixed_slopes_from_model()` function)

```{r compute-D-1, fig.cap="Posterior estimates for $D$."}
slopes1 <- extract_fixed_slopes_from_model(m_exp1_sft)
slopes2 <- extract_fixed_slopes_from_model(m_exp2_sft)
slopes3 <- extract_fixed_slopes_from_model(m_exp3_sft)
slopes4 <- extract_fixed_slopes_from_model(m_exp4_sft)

bind_rows(
  slopes1 %>% mutate(exp = "Experiment 1"),
  slopes3 %>% mutate(exp = "Experiment 3")) %>%
  ggplot(aes(x= D, fill = d_feature)) + 
  geom_density(alpha = 0.5) +
  facet_wrap(~exp, nrow = 2)
  
```

## Estimating model parameters

### $D$ for compound feature distractors

```{r predict-D-2, fig.cap = "Estimating D.", fig.height = 4}

Dp_samples <- bind_rows(
   get_Dp_samples(2, d, slopes1, slopes2),
   get_Dp_samples(4, d, slopes3, slopes4))



Dp_lines <- get_Dp_lines(Dp_samples)
knitr::kable(Dp_lines, digits = 2)
plot_Dp_lines(Dp_lines)

rm(slopes1, slopes2, slopes3, slopes4)

```

### Variance!

As we are using multi-level models to describe the whole distribution of reaction times we aslo need to provide a $\sigma$ and a sd(Intercept). 


Random intercepts look similar:

```{r}
VarCorr(m_exp1_sft)$p_id$sd
VarCorr(m_exp2_sft)$p_id$sd
```

Residual variance looks similar, although exp2 is lower that exp 1. 

```{r}
VarCorr(m_exp1_sft)$residual$sd
VarCorr(m_exp2_sft)$residual$sd
```


Seems legit for now to model Exp2 using the variances from Exp1 :)


## Predicting Reaction Times

```{r gen-predictions, fig.width = 10, fig.height = 10, cache=TRUE}
# in order to predict reaction times, we will specify a new version of our model
# It is a little bit hacky, but we can feed the predicted values for D into the 
# brms model as a prior, and then use that to simulate new trials.  

# first, summarise our Dp samples to give mu and sigma.
 Dp_samples %>%
   group_by(exp_id, d_feature, method) %>%
  summarise(mu = mean(Dp), sigma = sd(Dp), .groups = "drop") %>%
  mutate(d_feature = as_factor(d_feature)) -> Dp_summary

# now define and run new model! 
meth = "orthog_contrast"

model_params <- set_up_predict_model(2, "shifted_lognormal", meth, Dp_summary, m_exp1_sft, m_exp2_sft)
m_prt <- run_model(model_params, ppc = "only")

```

Note, this plot is for how we expect the average person to perform in experiment 2. 

```{r plot-exp2-predictions, fig.height = 6, fig.cap="reaction time predictions."}
plot_model_fits_rt(2, m_prt, y_limits = c(0, 1), n_row = 3, plot_type = "fitted")
```

Attempting plots for unknown participant! 

```{r plot-exp2-predictions-new-person, fig.height = 6, fig.cap="reaction time predictions."}
plot_model_fits_rt(2, m_prt, y_limits = c(0, 2), n_row = 3, plot_type = "predicted")
```

# Power Analyis for Planned Studies

What do the results look like if:

1) the same people do all parts of the study, and there is a correlation between tasks
2) We reduce the number of trials down to 96 per condition. 


## Pretend the original experiment was within subjects!


rename participants to pretend that we have a between subjects measure!

```{r}

d %>% 
  filter(exp_id %in% c(1, 2)) %>%
  select(-exp_id) %>% 
  separate(p_id, c("exp_id", "p_id"), "-") %>%
  mutate(p_id = paste(p_id, exp_id)) -> d

shuffle_noise = 0
map_dfr(unique(d$exp_id), rank_order_people, shuffle_noise = shuffle_noise) %>%
  left_join(d, by = "p_id") -> dr_no_shuffle 

dr_no_shuffle %>% group_by(ps_id, exp_id) %>% summarise(m_rt = median(rt), .groups = "drop") %>% 
  pivot_wider(names_from = "exp_id", values_from = "m_rt") %>%
  ggplot(aes(x = `1a`, y = `2c`)) + geom_point(colour = "cyan")  + 
  coord_fixed() -> plt_no_shuffle

shuffle_noise = 0.15
map_dfr(unique(d$exp_id), rank_order_people, shuffle_noise = shuffle_noise) %>%
  left_join(d, by = "p_id") -> dr_shuffle 

dr_shuffle %>% group_by(ps_id, exp_id) %>% summarise(m_rt = median(rt), .groups = "drop") %>% 
  pivot_wider(names_from = "exp_id", values_from = "m_rt") %>%
  ggplot(aes(x = `1a`, y = `2c`)) + geom_point(colour = "cyan") + 
  coord_fixed() -> plt_shuffle

plt_no_shuffle + plt_shuffle
```

We can get a better idea of our assumed correlations by calculating the correlation matrix between each participant's median RT

```{r}
dr_no_shuffle %>% group_by(ps_id, exp_id) %>% summarise(m_rt = median(rt), .groups = "drop") %>%
  pivot_wider(names_from = "exp_id", values_from = "m_rt") %>%
  select(-ps_id) -> dc

corrplot(cor(dc), method = "number") -> plt_c_ns

dr_shuffle %>% group_by(ps_id, exp_id) %>% summarise(m_rt = median(rt), .groups = "drop") %>%
  pivot_wider(names_from = "exp_id", values_from = "m_rt") %>%
  select(-ps_id) -> dc

corrplot(cor(dc), method = "number") -> plt_c_s

plt_c_ns + plt_c_s

```

If we now rerun the shifted-lognormal model on this dataset (Experiment 1 only) we can investigate how well we can predict each individual participant's performance in Experiment 2. 



## Sample 6 trials per condition


```{r, fig.height = 10}
# summary(d)
# 
# d %>% group_by(p_id, d_feature, N_T) %>%
#   slice_sample(n = 6) -> dss
# 
# d <- dss
# 
# summary(d)
# 
# mdl_inputs_log <- set_up_model(1, "lognormal")
# power_mdl1_log <- run_model(mdl_inputs_log, ppc = "no")
# 
# md_inputs_log <- set_up_model(2, "lognormal")
# power_mdl2_log <- run_model(mdl_inputs_log, ppc = "no")
# 
# mdl_inputs_log <- set_up_model(3, "lognormal")
# power_mdl3_log <- run_model(mdl_inputs_log, ppc = "no")
# 
# mdl_inputs_log <- set_up_model(4, "lognormal")
# power_mdl4_log <- run_model(mdl_inputs_log, ppc = "no")
```


```{r}

# plot_model_fits_rt(1, power_mdl1_log, plot_type = "fitted")
```


```{r compute-D-1-power, fig.cap="Posterior estimates for $D$."}
# slopes1 <- extract_fixed_slopes_from_model(power_mdl1_log)
# slopes3 <- extract_fixed_slopes_from_model(power_mdl3_log)
# 
# bind_rows(
#   slopes1 %>% mutate(exp = "Experiment1"),
#   slopes3 %>% mutate(exp = "Experiment 3")) %>%
#   ggplot(aes(x= D, fill = d_feature)) + 
#   geom_density(alpha = 0.5) +
#   facet_wrap(~exp, nrow = 2)
  
```


## Estimating model parameters

### $D$ for compound feature distractors

```{r predict-D-2-power}
# slopes2 <- extract_fixed_slopes_from_model(m_exp2_log)
# slopes4 <- extract_fixed_slopes_from_model(m_exp4_log)
# 
# 
# Dp_samples <- bind_rows(
#   get_Dp_samples(2, d, slopes1, slopes2),
#   get_Dp_samples(4, d, slopes3, slopes4))
# 
# Dp_lines <- get_Dp_lines(Dp_samples)
# knitr::kable(Dp_lines, digits = 2)
# plot_Dp_lines(Dp_lines)
# 
# ggsave("../plots/recreate_log_normal_fig_4.pdf", width = 8, height = 2.5)
# 
# rm(slopes1, slopes2)


```


# Enviroment Details, packages and Vesion Numbers


```{r session-info}
sessionInfo()
```