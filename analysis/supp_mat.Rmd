---
title: "Implementing Buetti et al(2019)"
author: "ADF Clarke"
date: "24/08/2020"
output: bookdown::html_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
   fig.height = 3,
  fig.align = "center")

library(tidyverse)
library(brms)
library(tidybayes)

source("reimplementation.R")
source("our_functions.R")

exps_to_predict = c("2a", "2b", "2c", "4a", "4b", "4c")
```
# Data Import and Overview

## Importing

We will import all experiments. While we're at it, we will remove error trials and very very short responses.

*What about very very long RTs? I'm not sure if there is a good reason to, or obvious cut off? Unlike for short Rts, in which values == 0 are mathematically impossible, and values <10ms are implausible. 
  
```{r import-data}
source("import_and_tidy.R")
 # remove error trials and very very short responses
d <- filter(d, error == 0, rt > 0.01)
```

## Overview: some summary statisitcs

```{r rt_exp_1b_hist, fig.cap="Histogram showing indivdidual differences in median RT in Experiment 1b."}
d %>% group_by(exp_id, p_id, d_feature) %>%
  summarise(median_rt = median(rt)) %>%
  filter(exp_id == "1b") %>%
  ggplot(aes(x = median_rt, fill = d_feature)) + 
  geom_histogram(bins = 10, colour = "black") + 
  facet_wrap(~d_feature) + 
  scale_x_continuous("median reaction time") +
  theme_bw()

ggsave("../plots/histograms_of_rt.pdf", width = 6, height = 3)
```

# Computational Replication of Buetti et al (2019)

Before doing anything else (i.e., new), we want to confirm that we can replicate the original analysis.

## Calculating $D$

Calculate $D_e$ for each condition in each experiment. 

```{r}
De <- map_dfr(unique(d$exp_id), calc_D_per_feature, d)
```

## Predicing $D$

We can now predict values of $D$ for all experiments 2x and 4x. 

```{r}
Dp <- map_df(c("2a", "2b", "2c", "4a", "4b", "4c"), gen_exp_predictions, De)
```
 
Now, recreate the scatter plots from Buetti et al (2019), Figure 4.

```{r replicating_D_plot, fig.cap="Computational replication of Figure 4 (top row) from Buetti et al (2019)."}
left_join(Dp, De, by = c("exp_id", "d_feature")) %>%
  pivot_longer(
    cols = c(`best feature`, `orthog. contrast`, collinear),
    values_to = "Dp",
    names_to = "method") %>%
  mutate(method = fct_relevel(method, "best feature", "orthog. contrast")) %>%
  ggplot(aes(x = Dp, y = D)) +
  geom_point() + 
  geom_abline(linetype = 2) +
  geom_smooth(method = "lm", formula = y ~ x, colour = "violetred3") + 
  coord_cartesian(xlim = c(0, 80), ylim = c(0, 80)) + 
  facet_wrap(~ method) + 
  see::theme_lucid() + 
  scale_x_continuous("predicted D") + 
  scale_y_continuous("empirical D")
```

## Predicting Reaction Times

$L$ indicates the number of distractor types present in the display, $N_T$ is the total number of distractors, $N_i$ is the number of distractors of type $i$, $D_j$ indicates the logarithmic slope parameters associated with distractor of type $j$ (organized from smallest $D_1$ to largest $D_L$). Note that the $D$ parameter is the one that increases with increasing target-distractor similarity.

The constant a represents the reaction time when the target is alone in the display. Inter-item interactions were indexed by the multiplicative factor $\beta$. Finally, the index function $1_{[2, \infty)} (j)$ indicates that the sum over Ni only applies when there are at least two different types of lures in the display $(j > 1)$. When $j = 1$, the second sum is zero.

```{r pred_rt_replication, fig.width = 4, fig.cap="Computational Replication of RT predictions."}
exps2predict = c("2a", "2b", "2c", "4a", "4b", "4c")

rt_pred <- map_dfr(exps2predict, predict_rt)

d %>% filter(exp_id %in% exps2predict) %>%
	group_by(exp_id, p_id, d_feature, N_T) %>%
	summarise(mean_rt = mean(rt), .groups = "drop") %>%
	group_by(exp_id,  d_feature, N_T) %>%
	summarise(mean_rt = mean(mean_rt), .groups = "drop") %>%
	left_join(rt_pred, by = c("exp_id", "d_feature", "N_T")) %>% 
	ggplot(aes(x = p_rt, y = mean_rt)) + 
  geom_point(alpha = 0.5) + 
  geom_abline() + 
  geom_smooth(method = "lm", formula = y ~ x, colour = "violetred3") + 
  see::theme_lucid() +
  scale_x_continuous("predicted reaction time (ms)") +
  scale_y_continuous("empirical mean reaction time (ms)")

# Exporting some data in case we need to sanity check
d %>% filter(exp_id %in% exps2predict) %>%
  group_by(exp_id, p_id, d_feature, N_T) %>%
  summarise(mean_rt = mean(rt), .groups = "drop") %>%
  group_by(exp_id,  d_feature, N_T) %>%
  summarise(mean_rt = mean(mean_rt), .groups = "drop") %>%
  left_join(rt_pred, by = c("exp_id", "d_feature", "N_T")) -> d_out
```

<TODO> show RT predictions for the other two methods?

# Switching to a Bayesian Multi-Level Framework

Now that we have verified that we can re-create the original results, we switch to a Bayesian multi-level framework. We make the following important changes:

  - Modelling trial data, rather than pooled mean reaction time data. This allows us to fit a model that can generate data at the trial level, and account 
  - Use a lognormal distribution for modelling reaction times, This allows us to avoid ever predicing impossible negative reaction times. It also helps us to account for the skew in the distribution
  - We will switch from using milli-seconds to seconds. This leaves us with most values around 0.5-1seconds, which will help model fitting. I.e., a more standardised scale. 
  
## Measuring $D$ from Empirical Data

```{r}
d %>% mutate(rt = rt/1000) -> d
```

### Prior Predictions

We can take the prior model, and then use it to compute our prior predictions!

```{r glmm-prior, cache=TRUE, fig.cap="Prior predictions for reaction time and log(N+1)."}
# prior_model <- fit_glmm_to_an_exp("1a", d, ppc = TRUE)
# plot_model_fits_ex(d, "1a", prior_model, 1)
```

### Fit Model and Posterior Predictions

```{r fit-glm-to-buetti2019-data, cache=TRUE}
# my_models <- map(unique(d$exp_id), fit_glmm_to_an_exp, d)
# 
# saveRDS(my_models, "my.models")
my_models <- readRDS("my.models")
```

```{r plot-glm-to-buetti2019-data, cache=TRUE, fig.cap="The multi-level fits for Experiment 2a (Buetti, 2019)."}
plot_model_fits_ex(d, "2a", my_models[[3]], 1:5)
```

```{r}
De <- map_df(
  1:length(my_models), 
  extract_fixed_slopes_from_model, 
  my_models, df = d)

ggplot(De, aes(x = D, fill = d_feature)) + 
  geom_density(alpha = 0.333) +
  facet_wrap(~ exp_id, ncol = 5) + 
  theme_bw()

ggsave("../plots/buetti2019_bayesian_fits.pdf", width = 8, height = 3)
```

## Estimating $D$ for compound feature distractors

```{r}

Dp_samples <- map_df(exps_to_predict, get_Dp_samples, d)

# add in average D (of collinear and orthogonal)

Dp_samples %>% pivot_wider(names_from = method, values_from = Dp) %>%
  group_by(iter, exp_id, d_feature) %>%
  mutate(mean_method = mean(c(orthog_contrast, collinear))) %>% 
  pivot_longer(c(best_feature, orthog_contrast, collinear, mean_method), names_to = "method", values_to = "Dp") -> Dp_samples


 Dp_samples %>% 
   group_by(exp_id, d_feature, method) %>%
   mean_hdci(Dp, De) %>%
  ggplot(aes(x = Dp, y = De)) + 
  geom_abline(linetype = 2) + 
  geom_point() +
  facet_wrap(~method, nrow = 1) + theme_bw() +
  geom_linerange(aes(ymin = De.lower, ymax = De.upper), alpha = 0.5) + 
  geom_linerange(aes(xmin = Dp.lower, xmax = Dp.lower), alpha = 0.5) + 
  geom_smooth(method = lm, colour = "pink")
 
ggsave("recreate_log_normal_fig_4.pdf", width = 8, height = 2.5)

```

## Which slope is closer to 1?

```{r}
slopes_err <- function(ii, df)  {
  
  d_itr = filter(df, iter == ii)
  
  beta = summary(lm(data = d_itr, De ~ (Dp):(0 + method)))$coefficients[, 1]
  beta_err = abs(1 - beta)
  
  return(beta_err)
}


beta_err <- map_df(unique(Dp_samples$iter), slopes_err, Dp_samples) 
names(beta_err) <- str_remove(names(beta_err), 'Dp:method')

 beta_err %>% pivot_longer(1:4, names_to = "method", values_to = "abs_res") %>%
   ggplot(aes(x = abs_res, fill = method)) + geom_density(alpha = 0.33)

 beta_err %>% select(-best_feature) %>%
   mutate(difference = orthog_contrast - collinear) %>%
   ggplot(aes(x = difference)) + geom_density()
 
  beta_err %>% select(-best_feature) %>%
   mutate(difference = mean_method - collinear) -> x
  
  mean(x$difference > 0)

```

## Predicting Reaction Times

```{r, fig.width = 10, fig.height = 10}

 Dp_samples %>% 
   group_by(exp_id, d_feature, method) %>%
  summarise(mu = mean(Dp), sigma = sd(Dp), .groups = "drop")-> Dp_summary

e_id <- "2a"
meth <- "mean_method"

d_prt <- map_df(exps_to_predict, predict_rt_b, meth, Dp_summary, d)

d_rt <- d %>% filter(N_T > 0) %>%
  group_by(p_id, exp_id, d_feature, N_T) %>% summarise(mean_rt = mean(rt)) %>%
   group_by(exp_id, d_feature, N_T) %>% summarise(mean_rt = mean(mean_rt)) %>%
  right_join(d_prt)

ggplot(data = d_rt, aes(x = mean_rt, y = .value)) + 
  geom_abline(linetype = 2) +
  geom_point() + geom_errorbar(aes(ymin = .lower, ymax = .upper))+ 
  theme_bw()
  

```
# Power Analyis for Planned Studies
