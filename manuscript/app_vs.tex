%\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
\documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
\usepackage{cite}
\usepackage{amsmath}
\usepackage{mathptmx}  
\usepackage{subfigure}    % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%B
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{The Alejandro Project: Testing the Target Contrast Signal Theory\thanks{ESRC grant?}
}
\subtitle{Replication and Generalisation}

\titlerunning{Testing the TCS Theory}        % if too long for running head

\author{Anna Hughes \and Anna Nowakowska \and Alasdair D. F. Clarke}

%\authorrunning{Short form of author list} % if too long for running head

\institute{F. Author \at
              first address \\
              Tel.: +123-45-678910\\
              Fax: +123-45-678910\\
              \email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           \and
           S. Author \at
              second address
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor

\maketitle

\begin{abstract}
Insert your abstract here. Include keywords, PACS and mathematical
subject classification numbers as needed.
\keywords{First keyword \and Second keyword \and More}
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}
\label{intro}

\paragraph{Background}
Visual search, where participants are asked to find a target within a cluttered scene, has been extensively studied within psychology. Several models have been developed that can generate testable predictions about how different types of distractors and targets affect search efficiency.

The two most studied families of models of visual search include bottom-up models (also known as visual saliency models) and top-down models. Saliency models rest on the assumption that fixations are directed to objects or locations that are most dissimilar to the background or other objects in the visual display \cite{itti2000saliency, itti1998model, koch1987shifts}. While the original saliency model was able to predict fixation allocation in a visual search task above chance \cite{parkhurst2002modeling}, further research demonstrated that a comparable level of performance could be achieved using a simple central fixation bias heuristic \cite{tatler2007central}. The saliency models have since been extended and improved (see for example \cite{zhang2008sun}): however, the main issue with this family of models remains their limited usability in complex real-life search arrays \cite{tatler2011eye, koehler2014saliency}. In addition, in most instances of visual search, the target is clearly defined (i.e. the goal is to find a specific object) and inspecting the most salient areas of the display may in these cases be inefficient.

Another class of models are based around Feature Integration Theory \cite{treisman1980feature}, which has been modified and extended by Wolfe in the Guided Search Model \cite{wolfe1989guided,wolfe2014approaches}. These models combine top-down influences (how closely an item resembles the observer's goal) with bottom-up image properties. For example, if one's goal (top-down processing) is to find a red horizontal bar, all the red and horizontal items in a visual search display will be given greater weight than distractors (e.g. vertical and blue items) in the model. The salience of a given object in the display (how distinctive it is from the surrounding objects) also activates bottom-up processing. For instance, a blue item among red items is ranked higher than red among orange items. Combining bottom-up and top-down sources of activation generates an activation map which generates a prediction of the order in which stimuli are processed in visual search. Thus, these models aim to produce a representation of the visual properties of the distractors at each location in the visual field. 
 
However, more recent work has taken a different approach, focusing solely on representing the difference between targets and distractors. For example, in work on eye movement patterns, it has been proposed that performance in inefficient (serial) visual search is mostly determined by the size of the 'functional viewing field', whose size varies as a function of target-distractor similarity \cite{hulleman2017brink}. One recent model, the Target Contrast Signal (TCS) Theory \cite{lleras2020target} aims to provide a unifying, quantitative framework that can make behavioural predictions based on this general assumption.

\paragraph{Theoretical intro to TCS}
TCS proposes that behaviour is determined by comparing the target template (held in memory) with every element present in the scene in parallel. This allows the visual system to reject peripheral non-targets quickly; the speed at which items are evaluated is determined by how different the item is from the template through an evidence accumulation process (formally, the slope of the logarithmic function is assumed to be inversely proportional to the overall magnitude of the contrast signal between the target and distractor). The model thus focuses on an initial, efficient processing stage of search; if sufficient evidence is not accumulated during this process, the model posits that a second stage is entered, where attention is deployed serially. TCS has been successful in predicting a number of empirical results, including search performance in heterogeneous scenes based on parameters estimated in homogeneous scenes, both with artificial stimuli \cite{buetti2016towards,lleras2019predicting} and with real-world objects visualised on a computer display \cite{wang2017predicting}. 
 
\paragraph{Limitations, and extending basic stimuli} While results to date for TCS appear promising, they remain relatively preliminary, being tested by only one research group so far. There remains a great deal of scope for extending beyond the parameters tested to date (such as the number of distractors) in order to test the robustness and generalisability of the model. In addition, there are some predictions of the model, including its ability to explain search asymmetries, that have yet to be empirically confirmed \cite{lleras2020target}.

\paragraph{Within subjects}
In addition, in all implementations of TCS so far, predictions of search efficiency (e.g. in heterogeneous scenes) have been made on the average of a group of participants, using data from a different group performing a different task (e.g. searching in homogeneous scenes). Thus, we know that TCS can replicate group-level averages between subjects in search well, but we do not know whether it is also able to make predictions at the individual level. This is particularly important given that conclusions based on aggregate data can be different from those that take individual differences into account; in one study where participants searched for a target in an array of randomly oriented line segments, aggregating the data suggested that participants were using a stochastic search model. However, when considering each participant individually, it became clear that there was a high level of heterogeneity in responses, with some participants performing close to optimally, and others actually performing worse than chance \cite{nowakowska2017human}. Similarly striking variability has also been reported in other search studies \cite{irons2016choosing, irons2018characterizing}. 

\paragraph{Conclusion to introduction} 
In the current manuscript, we focus on replicating and extending findings from \cite{buetti2019predicting}. Here, participants searched for a target in a scene of homogeneous distractors. First, parallel search efficiency (measured by the logarithmic search slope) was estimated for cases where the distractors varied from the target in one dimension: either colour (e.g. a cyan target being searched for in either yellow, blue or orange distractors) or shape (e.g. a semicircle target in either circle, diamond or triangle distractors). New participants then searched for the same targets in displays where the distractors were compounds, differing from the target in both colour and shape (e.g. searching for a cyan semicircle in either blue circles, orange diamonds or yellow triangles). Figure \ref{fig:buetti2019_stimulus} shows example stimuli from their paper. The logarithmic search slopes in the initial experiments were then used to predict the logarithmic slopes and reaction times using a number of models. The authors found that the best model was a 'collinear contrast integration model' where the distinctiveness scores were summed along each attribute in the unidimensional experiments, creating an overall contrast score that was used for compound stimuli predictions.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../plots/example_stimuli_figure.pdf}
\caption{Example stimuli from \cite{buetti2019predicting} Top left: Expt 1A. Here, the target is a blue semicircle within a set of homogeneous (yellow semicircle) distractors. Top right: Expt 1B. The target is a grey semicircle in circular grey distractors. Bottom left: Expt 2A. The target is a blue semicircle in orange diamond distractors. Bottom middle: Expt 2B. The target is a blue semicircle in dark blue triangle distractors. Bottom right: Expt 2C. The target is a blue semicircle in yellow circular distractors.}
\label{fig:buetti2019_stimulus}
\end{figure}

\paragraph{Overview of the current study}
We first run a replication of \cite{buetti2019predicting}, in an online, within-subjects study. This design allows us to extend the modelling, both incorporating a multi-level design to predict within-subjects effects and by utilising a Bayesian generalised linear model framework to better represent the distribution of responses (e.g. avoiding predicting negative reaction times, accounting for uncertainty in model predictions). We  also carry out a direct analytical replication using the same methods as in \cite{buetti2019predicting} allowing us to ask whether the choice of analysis affects the results.

\section{A multi-level Bayesian reanalysis}
\label{sec:reanalysis}

We first describe the original analysis methods, and then our modifications. 

\subsection{TCS modelling overview} 

According to TCS, Equation \ref{eq:buetti2019} allows us to predict mean reaction times:

\begin{equation}
RT = a + \beta\sum_{j=1}^L\left((D_j - D_{j-1})\log\left(\left(N_T - \sum_{i=1}^{j-1}N_i\right)1_{[2,\infty](j)}+1 \right)\right)
\label{eq:buetti2019}
\end{equation}

Where $L$ is the number of distractor types present in the display, $N_T$ is the total number of distractors, $N_i$ is the number of distractors of type $i$ and $D_j$ is the logarithmic slope parameters associated with distractors of type $j$ (from smallest $D_1$ to largest $D_L$). The constant $a$ represents the reaction time when no distractors are present and $\beta$ represents inter-item interactions. When $L=1$ (all lures are identical), as in the current experiment, the equation simplifies to:

\begin{equation}
RT = a + D_1\log(N_T+1)
\end{equation}

\subsubsection{Estimating $D$}

\cite{buetti2019predicting} tested three different models for predicting $D$ for compound colour-shape stimuli. The best feature guidance model (Equation \ref{eq:bestfeature}) suggests that when the target and lures differ in two dimensions, participants will choose to attend to whichever feature dimension is the most discriminable (i.e. has the smallest $D$ value):

\begin{equation}
D_\text{overall} = \text{min}\left(D_\text{colour}, D_\text{shape}\right)
\label{eq:bestfeature}
\end{equation}

The orthogonal contrast combination model instead suggests that independent feature dimensions comprise a multidimensional space, where an object can be described by the overall vector in this space, and thus $\mathrm{D_{overall}}$ can be represented as:

\begin{equation}
D_\text{overall} = \frac{1}{\sqrt{\frac{1}{(D_\text{colour})^2 + (D_\text{shape})^2}}}
\label{eq:orthogonalcontrast}
\end{equation}

Which simplifies as:

\begin{equation}
D_\text{overall} = (D_\text{colour})^2 + (D_\text{shape})^2
\label{eq:orthogonalcontrast2}
\end{equation}

Finally, the collinear contrast integration model also assumes independence of feature dimensions, but assumes that while the visual features create a multidimensional space, the contrast between them is unidimensional. As $D$ is assumed to be inversely proportional to contrast, the equation can be written as follows:

\begin{equation}
\frac{1}{D_\text{overall}} = \frac{1}{D_\text{colour}} + \frac{1}{D_\text{shape}}
\label{eq:collinearcontrast}
\end{equation}

\cite{buetti2019predicting} found that with their dataset, the collinear contrast integration model was best able to predict their empirical RT results with an $R^2$ = 93.27%.

We verified we were able to replicate the results of \cite{buetti2019predicting} using the dataset available on OSF (https://osf.io/f3m24/)\footnote{downloaded on 28th August 2020} and using the exclusion criteria originally applied; see \textit{Supplementary Materials} for details. 

\subsubsection{Discussion}

We did it! 

But \ldots 

insert a figure showing trial data points plotted around the model's point prediction to justify switch to multi-levels and log normal. 


\subsection{Our Re-analysis}

Some of the weaknesses of the Target Contrast Signal Theory are i) it is unable to account for individual differences, only the changes to the sample average and  ii) it  cannot account for the distribution of reaction times over multiple trials. Switching from a simple linear regression model to a multi-level model will allow us to compute $D$ for each participant, and estimate the trial-to-trial variance. By also switching from a frequentist to Bayesian framework, we can also account for the uncertainty in the model's predictions.

First, we computed the $D$ values using a multi-level Bayesian model using a log-normal distribution (using R, brms etc. - see Equation \ref{eq:computeDlm}). Using this method allows us to fit each individual's performance. This is important because it is clear there are between-subjects differences that are not captured in the original model. 
%(see Figure \ref{fig:buetti2019_hist}). 

%\begin{figure}
%\centering
%\includegraphics[width=\textwidth]{../plots/histograms_of_rt.pdf}
%\caption{Histograms of median RTs from Expt 3A in \cite{buetti2019predicting}). There are clearly between-subjects differences, as well as differences in distributions for the different conditions.}
%\label{fig:buetti2019_hist}
%\end{figure}

We fit the model twice, once using a log link\footnote{our preference, as this avoids predicting negative reaction times and helps to account for skew in the distribution}, and once using the identity link\footnote{makes this new version of the model more comparable to the original}. Other minor changes include modelling reaction times on the seconds scale, rather than milliseconds (making numerical model fitting easier) and removing all RTs less than 25ms.

\begin{equation}
\hat{y} = a + D\log(N_T + 1)
\label{eq:computeDlm}
\end{equation}

The code for our model in \textit{brms} is as follows (possibly this can be a suppmat thing?):

\begin{verbatim}
m <- brm(
    rt ~  0 + d_feature + log(N_T+1):d_feature + 
      (log(N_T+1):d_feature|p_id),
    data = df,
    family = lognormal(link = "identity"),
    prior = my_priors,
    iter = 5000)
\end{verbatim}


\begin{figure}
\centering
\subfigure{\includegraphics[width=\textwidth/0.4]{../wd_reanalyse_Buetti2019/empirical_D_examples.png}}
\caption{The shaded regions show the model's predicted D values for each of the conditions in Expt 1A.}
\label{fig:buetti2019_a1}
\end{figure}

\subsubsection{Measuring $D$}

As we are using a Bayesian framework, rather than $D$ representing the best fit in equation \ref{eq:computeDlm}, it is now represented as a probability distribution over possible values (see Figure \ref{fig:buetti2019_a1}). We can can then estimate $D$ in the compound feature distractor cases using the three methods detailed above and compare the predicted values to the empirical values - see Figure \ref{fig:buetti2019_D}. Interestingly, the choice of distribution seems to have an effect on which model gives the best prediction; for the normal distribution (as used in \cite{buetti2019predicting}) the collinear contrast integration model gives the best fit to the data (as with the original analysis), but for the log-normal distribution, the orthogonal contrast model is better.

\begin{figure}
\centering
\subfigure{\includegraphics[width=\textwidth]{../wd_reanalyse_Buetti2019/recreate_normal_fig_4.pdf}}
\subfigure{\includegraphics[width=\textwidth]{../wd_reanalyse_Buetti2019/recreate_log_normal_fig_4.pdf}}
\caption{Correlation between empirical $D$ and the predicted $D$ using \textit{(top)} normal and (\textit{bottom}) log-normal distributions.}
\label{fig:buetti2019_D}
\end{figure}

\subsubsection{Estimating $D$ for compound distractors}

\subsubsection{Reaction Time Model}

\subsection{Extension: Individual Differences}

While we can't generate within-participant reaction time predictions from the original data, we can present data on the uncertainty around the individual differences in $D$ for each condition (see Figure \ref{fig:buetti2019_people}).

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../wd_reanalyse_Buetti2019/exp1_fits.png}
\caption{Example results from the first five participants of Experiment 1a \cite{buetti2019predicting}. We can see considerable between-subjects variation. }
\label{fig:buetti2019_people}
\end{figure}

\subsection{Power Analysis}
\label{sec:power}

To estimate the required sample size for future experiments, we repeatedly reran the above analysis on smaller subsets of the data from \cite{buetti2019predicting}. (Full details are given in the \textit{Supplementary Materials}.) Based on this, we found that $n = $ per observer and condition was enough to reliably estimate $D$. 

\subsection{Discussion}

While we replicate the good predictions using the collinear method if we use a normal distribution, we do not find this that this is the best method if we use a log-normal distribution, and instead, we find the orthogonal contrast method is slightly better. This suggests that (seemingly small) analytical decisions may have important implications for the conclusions drawn. 

\section{Hypotheses}

We plan a number of experiments to test the extent to which the original results in \cite{buetti2019predicting} replicate and generalise. As well as following the original, between-subjects, experiment design, in which data from one group of observers in one task is used to predict behaviour of a second group of participants in a different task, we will allow for within-subject comparisons. Specifically, we will ask: to what extent do the individual differences in the homogeneous task explain the differences in the heterogeneous task? 

\begin{enumerate}
\item \textbf{Replication of Buetti et al (2019) with online data collection.} Specifically, that the \textit{collinear contrast ingratiation model} outperforms the \textit{best feature guidance}, and \textit{orthogonal contrast combination models}.  Furthermore, the $R^2 = $ ($99\%$ HPDI = $[, ]$) between predicted and observer reaction times.\\
\item \textbf{Larger number of distractors (or targets further in periphery?)} Add an extra ring \\ 
\item \textbf{Search asymmetries} test O in Qs and Q in Os \\
\end{enumerate}

\section{General Methods}

\subsection{Participants}

Based on the power analysis \ref{sec:power}, we will recruit \ldots

\subsection{Stimuli}

\subsection{Procedure}

\subsection{Data Pre-processing}

incorrect trials? Poorly behaved participants? RTs that are far too short? Or far too long?

\subsection{Analysis Plan}

We will follow the analysis given in Section \ref{sec:reanalysis}.


\section{Experiment 1}



\subsection{Results}
\begin{center}
\textit{-- blank --}
\end{center}

\subsection{Discussion}
\begin{center}
\textit{-- blank --}
\end{center}

\section{Experiment 2}

\subsection{Methods}

Same as Experiment 1? Perhaps we should have a \textit{General Methods} section for both experiments?

\subsection{Results}
\begin{center}
\textit{-- blank --}
\end{center}

\subsection{Discussion}
\begin{center}
\textit{-- blank --}
\end{center}

\section{General Discussion}

Is discriminating between the different models one of our aims? Or is this a discussion point i.e. it's quite hard to do? And therefore maybe a follow up paper?


\begin{acknowledgements}
Thank you to AL for help and encouragement! 
\end{acknowledgements}

% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
\section*{Conflict of interest}
The authors declare that they have no conflict of interest.

% BibTeX users please use one of
\bibliographystyle{plain}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
%\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{sources}   % name your BibTeX data base

\end{document}