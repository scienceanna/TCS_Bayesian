# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1:10, N_T = full_seq(N_T,1))
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_nrl
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n == e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1:10, N_T = full_seq(N_T,1))
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(exp_id, N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_nrl
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n == e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1:10, N_T = full_seq(N_T,1))
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(exp_id, N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(exp_id, N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_nrl
plt_sft <- plot_model_fits_rt(training_models, m_exp1_sft, plot_type = "fitted")
plt_sft
#### Set up and data import ####
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(magrittr)
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# functions used for the analysis implementation
source("../scripts/reimplementation.R")
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# import and tidy data
source("../scripts/import_and_tidy.R")
# switch from ms to seconds
# Recode experiment as 1, 2, 3 and 4
d <- our_changes_to_data(d)
training_models <- c("1a", "1b")
#
# #### Prior fitting ####
# mdl_inputs_nrl <- set_up_model(training_models, "normal")
# prior_model_nrl <- run_model(mdl_inputs_nrl, ppc = "only")
# saveRDS(prior_model_nrl, "models/prior_nrl.models")
#
# mdl_inputs_log <- set_up_model(training_models, "lognormal")
# prior_model_log <- run_model(mdl_inputs_log, ppc = "only")
# saveRDS(prior_model_log, "models/prior_log.models")
#
# mdl_inputs_sft <- set_up_model(training_models, "shifted_lognormal")
# prior_model_sft <- run_model(mdl_inputs_sft, ppc = "only")
# saveRDS(prior_model_sft, "models/prior_sft.models")
#
# rm(
#   prior_model_nrl,
#   prior_model_log,
#   prior_model_sft)
#### Fit model for Expt 1&3 ####
training_models <- c("1a", "1b")
mdl_inputs_nrl <- set_up_model(training_models, "normal")
m_exp1_nrl <- run_model(mdl_inputs_nrl, ppc = "no")
saveRDS(m_exp1_nrl, "models/exp_1_nrl.models")
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
prior_model_nrl <- readRDS("models/prior_nrl.models")
prior_model_log <- readRDS("models/prior_log.models")
prior_model_sft <- readRDS("models/prior_sft.models")
plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
training_models = c("1a", "1b")
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_nrl
mdl_inputs_log <- set_up_model(training_models, "lognormal")
m_exp1_log <- run_model(mdl_inputs_log, ppc = "no")
saveRDS(m_exp1_log, "models/exp_1_log.models")
mdl_inputs_sft <- set_up_model(training_models, "shifted_lognormal")
m_exp1_sft <- run_model(mdl_inputs_sft, ppc = "no")
saveRDS(m_exp1_sft, "models/exp_1_sft.models")
training_models <- c("3a", "3b")
mdl_inputs_nrl <- set_up_model(training_models, "normal")
m_exp3_nrl <- run_model(mdl_inputs_nrl, ppc = "no")
saveRDS(m_exp1_nrl, "models/exp_3_nrl.models")
mdl_inputs_log <- set_up_model(training_models, "lognormal")
m_exp3_log <- run_model(mdl_inputs_log, ppc = "no")
saveRDS(m_exp1_log, "models/exp_3_log.models")
mdl_inputs_sft <- set_up_model(training_models, "shifted_lognormal")
m_exp3_sft <- run_model(mdl_inputs_sft, ppc = "no")
saveRDS(m_exp1_sft, "models/exp_3_sft.models")
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
prior_model_nrl <- readRDS("models/prior_nrl.models")
prior_model_log <- readRDS("models/prior_log.models")
prior_model_sft <- readRDS("models/prior_sft.models")
#plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
#plt_log <- plot_model_fits_rt(1, prior_model_log, y_limits = c(0, 10), feature2plot = "blue")
#plt_sft <- plot_model_fits_rt(1, prior_model_sft, y_limits = c(0, 10), feature2plot = "blue")
# plt_nrl + plt_log + plt_sft
# tidy up, we no longer need to keep hold of these models and plots
rm(
prior_model_nrl,
prior_model_log,
prior_model_sft,
plt_nrl, plt_log, plt_sft)
m_exp1_nrl <- readRDS("models/exp_1_nrl.models")
m_exp1_log <- readRDS("models/exp_1_log.models")
m_exp1_sft <- readRDS("models/exp_1_sft.models")
#m_exp1_sft_nolog <- readRDS("models/exp_1_3_sft_nolog.models")
training_models = c("1a", "1b")
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_log <- plot_model_fits_rt(training_models, m_exp1_log, plot_type = "fitted")
plt_sft <- plot_model_fits_rt(training_models, m_exp1_sft, plot_type = "fitted")
#plt_sft_nolog <- plot_model_fits_rt(1, m_exp1_sft_nolog, plot_type = "fitted")
plt_nrl / plt_log / plt_sft #/ plt_sft_nolog
rm(plt_nrl, plt_log, plt_sft, plt_sft_nolog)
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
traceback()
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n == e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1:10, N_T = full_seq(N_T,1))
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(exp_id, N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(exp_id, N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n == e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1, N_T = full_seq(N_T,1))
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(exp_id, N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(exp_id, N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
e_id = training_models
e_id
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
d_plt
d_plt %>%
expand(d, nesting(exp_id, d_feature), p_id = 1, N_T = full_seq(N_T,1))
d_plt %>%
expand(nesting(exp_id, d_feature), p_id = 1, N_T = full_seq(N_T,1))
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(nesting(exp_id, d_feature), p_id = 1, N_T = full_seq(N_T,1)) %>%
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_id %in% e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n == e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id),
exp_id = fct_drop(exp_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
expand(nesting(exp_id, d_feature), p_id = 1, N_T = full_seq(N_T,1)) %>%
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T, exp_id) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
expand(nesting(exp_id, d_feature), N_T = full_seq(N_T,1)) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(exp_id, N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(exp_id, N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
plt_log <- plot_model_fits_rt(training_models, m_exp1_log, plot_type = "predicted", y_limits = c(0, 2.5))
plt_sft <- plot_model_fits_rt(training_models, m_exp1_sft, plot_type = "predicted", y_limits = c(0, 2.5))
plt_nrl / plt_log / plt_sft / plt_sft_nolog
plt_nrl / plt_log / plt_sft #/ plt_sft_nolog
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
prior_model_nrl <- readRDS("models/prior_nrl.models")
prior_model_log <- readRDS("models/prior_log.models")
prior_model_sft <- readRDS("models/prior_sft.models")
#plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
#plt_log <- plot_model_fits_rt(1, prior_model_log, y_limits = c(0, 10), feature2plot = "blue")
#plt_sft <- plot_model_fits_rt(1, prior_model_sft, y_limits = c(0, 10), feature2plot = "blue")
# plt_nrl + plt_log + plt_sft
# tidy up, we no longer need to keep hold of these models and plots
rm(
prior_model_nrl,
prior_model_log,
prior_model_sft,
plt_nrl, plt_log, plt_sft)
m_exp1_nrl <- readRDS("models/exp_1_nrl.models")
m_exp1_log <- readRDS("models/exp_1_log.models")
m_exp1_sft <- readRDS("models/exp_1_sft.models")
#m_exp1_sft_nolog <- readRDS("models/exp_1_3_sft_nolog.models")
training_models = c("1a", "1b")
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "fitted")
plt_log <- plot_model_fits_rt(training_models, m_exp1_log, plot_type = "fitted")
plt_sft <- plot_model_fits_rt(training_models, m_exp1_sft, plot_type = "fitted")
#plt_sft_nolog <- plot_model_fits_rt(1, m_exp1_sft_nolog, plot_type = "fitted")
plt_nrl / plt_log / plt_sft #/ plt_sft_nolog
rm(plt_nrl, plt_log, plt_sft, plt_sft_nolog)
plt_nrl <- plot_model_fits_rt(training_models, m_exp1_nrl, plot_type = "predicted", y_limits = c(0, 2.5))
plt_log <- plot_model_fits_rt(training_models, m_exp1_log, plot_type = "predicted", y_limits = c(0, 2.5))
plt_sft <- plot_model_fits_rt(training_models, m_exp1_sft, plot_type = "predicted", y_limits = c(0, 2.5))
# plt_sft_nolog <- plot_model_fits_rt(1, m_exp1_sft_nolog, plot_type = "predicted", y_limits = c(0, 2.5))
plt_nrl / plt_log / plt_sft #/ plt_sft_nolog
rm(plt_nrl, plt_log, plt_sft, plt_sft_nolog)
bs_exp1_nrl <- bridge_sampler(m_exp1_nrl, silent = TRUE)
bs_exp1_log <- bridge_sampler(m_exp1_log, silent = TRUE)
bs_exp1_sft <- bridge_sampler(m_exp1_sft, silent = TRUE)
#bs_exp1_sft_nolog <- bridge_sampler(m_exp1_sft_nolog, silent = TRUE)
tibble(model = c("normal", "lognormal", "shifted-lognormal"), #, "linear in $N_T$"
weight = post_prob(bs_exp1_nrl, bs_exp1_log, bs_exp1_sft)) %>% # , bs_exp1_sft_nolog
knitr::kable()
#remove the models that we no longer need
rm(m_exp1_nrl, m_exp1_log, m_exp1_sft_nolog)
summary(m_exp1_sft)
plot(m_exp1_sft, pars = "^b_")
m_exp3_nrl <- readRDS("models/exp_3_nrl.models")
m_exp3_log <- readRDS("models/exp_3_log.models")
m_exp3_sft <- readRDS("models/exp_3_sft.models")
#m_exp1_sft_nolog <- readRDS("models/exp_1_3_sft_nolog.models")
plt_nrl <- plot_model_fits_rt(training_models, m_exp3_nrl, plot_type = "fitted")
plt_log <- plot_model_fits_rt(training_models, m_exp3_log, plot_type = "fitted")
plt_sft <- plot_model_fits_rt(training_models, m_exp3_sft, plot_type = "fitted")
plt_nrl / plt_log / plt_sft #/ plt_sft_nolog
