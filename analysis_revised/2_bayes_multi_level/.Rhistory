knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
d0 <-  filter(d, N_T == 0)
ggplot(d0, aes(x = rt, fill = exp_id)) + geom_density(alpha = 0.2) +
xlim(0, 2)
my_f <- bf(rt ~ 0 + exp_id + (1|p_id),
ndt ~ 1 + (1|p_id))
my_inits <- list(list(Intercept_ndt = -10), list(Intercept_ndt = -10), list(Intercept_ndt = -10), list(Intercept_ndt = -10))
my_prior <- c(
prior_string("normal(0, 0.2)", class = "b"),
prior_string("normal(-1, 0.5)", class = "Intercept", dpar = "ndt" ),
prior_string("cauchy(0, 0.4)", class = "sigma"),
prior_string("cauchy(0, 0.05)", class = "sd"),
prior_string("cauchy(0, 0.05)", class = "sd", dpar = "ndt"))
a_m <- brm(my_f,
data = d0,
prior = my_prior,
inits =my_inits,
chains = 4,
iter = 5000,
family = shifted_lognormal())
readRDS("models/intercepts.models")
saveRDS("models/intercepts.models")
saveRDS(a_m, "models/intercepts.models")
summary(a_m)
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
d <- our_changes_to_data(d)
d
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
prior_model_nrl <- readRDS("models/prior_nrl.models")
prior_model_log <- readRDS("models/prior_log.models")
prior_model_sft <- readRDS("models/prior_sft.models")
plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
plot_model_fits_rt <- function(e_id, m, plot_type = 'predicted', y_limits = c(0, 1.5), n_row = 1, feature2plot = 'all', dot_col = "yellow1") {
# plot search slopes for experiment e_id
# take the experiment we want, and remove the N_T == 0 case.
if (feature2plot == "all") {
d %>%
filter(
exp_n == e_id) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id)) %>%
ungroup() -> d_plt
} else {
d %>%
filter(
exp_n== e_id, d_feature == feature2plot) %>%
mutate(
d_feature = fct_drop(d_feature),
p_id = fct_drop(p_id)) -> d_plt
}
if (plot_type == "predicted") {
# include all group-level effects.
# Let's simulate 100 new people!
d_plt %>%
modelr::data_grid(N_T = seq(0,36,4), d_feature, p_id = 1:100)  %>%
add_predicted_draws(m, re_formula = NULL, allow_new_levels = TRUE, n = 100) %>%
ungroup() %>%
select(-p_id) %>%
group_by(d_feature, N_T) -> d_hdci
} else {
# no group-level effects are included, so we are plotting
# for the average participant
d_plt %>%
modelr::data_grid(N_T = seq(0,36,4), d_feature) %>%
add_fitted_draws(m, re_formula = NA, scale = "response", n = 500) -> d_hdci
# we will plot these against the mean mean rt
d_plt %>% group_by(N_T, d_feature, p_id) %>%
summarise(mean_rt = mean(rt), .groups = "drop") %>%
group_by(N_T, d_feature) %>%
summarise(mean_rt = mean(mean_rt), .groups = "drop") -> d_plt
}
# calc 53% and 97% intervals for the model
d_hdci %>% mean_hdci(.width = c(0.53, 0.97)) -> d_hdci
plt <- plot_ribbon_quantiles(d_hdci, d_plt, y_limits, n_row, plot_type, dot_col)
return(plt)
}
plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
knitr::opts_chunk$set(
echo = TRUE,
fig.height = 3,
fig.align = "center")
library(tidyverse)
library(brms)
library(tidybayes)
library(patchwork)
library(latex2exp)
# set ggplot2 theme
theme_set(see::theme_abyss())
# use parallel cores for mcmc chains!
options(mc.cores = parallel::detectCores())
# reduce the number of decimal places
options(digits = 3)
# functions used for our Bayesian re-analysis
source("../scripts/our_functions.R")
# set seed to make sure everything is reproducible
set.seed(100320021)
source("../scripts/import_and_tidy.R")
summary(d)
# switch from ms to seconds
# recode experiment as 1, 2, 3 and 4
# remove outlier RTs
d <- our_changes_to_data(d)
prior_model_nrl <- readRDS("models/prior_nrl.models")
prior_model_log <- readRDS("models/prior_log.models")
prior_model_sft <- readRDS("models/prior_sft.models")
plt_nrl <- plot_model_fits_rt(1, prior_model_nrl, y_limits = c(-2, 10),feature2plot = "blue")
plt_log <- plot_model_fits_rt(1, prior_model_log, y_limits = c(0, 10), feature2plot = "blue")
plt_sft <- plot_model_fits_rt(1, prior_model_sft, y_limits = c(0, 10), feature2plot = "blue")
